{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xtz388eFVyo7"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pprint\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Editing of XML files\n",
        "\n",
        "contributors/collaborators: Peter Nadel"
      ],
      "metadata": {
        "id": "iSWkFRttsJn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initial XML\n",
        "!wget https://raw.githubusercontent.com/gregorycrane/Wolf1807/master/wolf.aw.xml\n",
        "wolf= open('wolf.aw.xml')\n",
        "wolf_soup = BeautifulSoup(wolf,'lxml')\n",
        "print(wolf_soup) "
      ],
      "metadata": {
        "id": "0MyJVAQ5sJNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using beautiful soup to find all the p or paragrapg tags \n",
        "paragraphs = wolf_soup.find_all('p')[0].find_all('p')\n",
        "paragraphs = paragraphs[3:] ## cleaning out other tags\n",
        "\n",
        "#numbering paragraphs using the index \n",
        "for index, ptag in enumerate(paragraphs): \n",
        "  ptag['id'] = f'para{index}'\n",
        "\n",
        "#using nltk to split the paragraphs into sentences \n",
        "#using the same beautiful soup process except creating new s tags for sentences \n",
        "#using the index of the sentence to number them\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "for p_index, paragraph in enumerate(paragraphs):\n",
        "  sentences = sent_tokenize(paragraph.get_text())\n",
        "  paragraph.clear()\n",
        "  for index, sentence in enumerate(sentences):\n",
        "    sent_tag = wolf_soup.new_tag('s', id= f'para{p_index}s{index}') \n",
        "    sent_tag.string = sentence \n",
        "    paragraph.append(sent_tag) \n",
        "\n",
        "paragraphs"
      ],
      "metadata": {
        "id": "VR6VgKZesXlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paragraphs output became the initial working xml file.\n",
        "It required some manual edits....\n",
        "Things I did manually:\n",
        "- Some of the sentence tokenization split up initials into individual sentences like A.b. Milne was split into three sentences\n",
        "- I had to manually re add in the notes tags \n",
        "\n",
        "Final XML can be found here : [https://github.com/jdeen33/Altertumswissenschaft-/blob/main/wolf_attributes.xml]"
      ],
      "metadata": {
        "id": "G9DO8IUbtuXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating ConLLu Files From Each Sentence**\n",
        "\n",
        "contributors/collaborators: Peter Nadel"
      ],
      "metadata": {
        "id": "amswf0_ivf2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "!python -m spacy download de_core_news_md \n",
        "nlp = spacy.load(\"de_core_news_md\")\n",
        "!pip install spacy_conll\n",
        "from spacy_conll import init_parser\n",
        "# include spacy-stanza and spacy-udpipe\n",
        "!pip install spacy_conll[parsers]\n",
        "# include pandas\n",
        "!pip install spacy_conll[pd]\n",
        "!pip install spacy_conll[all]\n",
        "from spacy_conll import init_parser\n",
        "\n",
        "nlp = init_parser(\"de\",\n",
        "                  \"stanza\",\n",
        "                  parser_opts={\"use_gpu\": True, \"verbose\": False},\n",
        "                  include_headers=True)\n",
        "config = {\"ext_names\": {\"conll_pd\": \"pandas\"},\n",
        "          \"conversion_maps\": {\"deprel\": {\"nsubj\": \"subj\"}}}"
      ],
      "metadata": {
        "id": "r84cKZpjvsKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making a dictionary w/ tag id as key\n",
        "#get_text as value \n",
        "#for every sentence in Wolf \n",
        "wolf= open('wolf_attributes.xml')\n",
        "wolf_soup = BeautifulSoup(wolf,'lxml')\n",
        "\n",
        "sents = wolf_soup.find_all('s')\n",
        "s_dict= {}\n",
        "for item in sents:\n",
        "  s_dict[item['id']] = item.get_text()\n",
        "\n",
        "s_dict"
      ],
      "metadata": {
        "id": "a8JQQARjv4xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert dictionary to pandas dataframe\n",
        "import pandas as pd\n",
        "wolf_s_tags = pd.DataFrame.from_dict(s_dict, orient='index').rename(columns={0:'StringText'},)\n",
        "wolf_s_tags.reset_index().rename(columns={'index':'p_id'})\n",
        "wolf_s_tags\n",
        "#making directory to store pandas files \n",
        "!mkdir pandas_csv"
      ],
      "metadata": {
        "id": "xwWctT7-wAm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I used a for loop w/ range to do this, which is very inefficient \n",
        "# but it kept throwing errors when a sentence tag was empty and interrupting the entire process\n",
        "#i think if I rewrote it with a try/except loop it would work\n",
        "#but anyways this uses spacy to convert the tokenizer output into a pandas dataframe,which is then converted into csv \n",
        "for i in range (901,969):\n",
        "  x =wolf_s_tags['StringText'][i]\n",
        "  doc = nlp(x)\n",
        "  s = doc._.conll_pd\n",
        "  s.to_csv(f\"pandas_csv1/{wolf_s_tags.index[i]}.csv\")"
      ],
      "metadata": {
        "id": "tOVPZcK0w3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#used python 'glob' tool to grab all the conllu csv files from the directory and make them into one huge dataframe \n",
        "import glob\n",
        "import pandas as pd\n",
        "sent_files = sorted(glob.glob('/content/pandas_csv1/para*.csv'))\n",
        "sent_files\n",
        "second_half = pd.concat((pd.read_csv(file) for file in sent_files))"
      ],
      "metadata": {
        "id": "wXTigwa5xUnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file can be found here :[https://github.com/jdeen33/Altertumswissenschaft-/blob/main/wolf_sents_entire]\n",
        "\n",
        "it needs to be updated w/ the new xml file, which I will do soon"
      ],
      "metadata": {
        "id": "92u325J_yy0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scraping Google Ngrams**\n",
        "\n",
        "sources used: https://www.geeksforgeeks.org/scrape-google-ngram-viewer-using-python/"
      ],
      "metadata": {
        "id": "IFUSG7v5znYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "YK-XvCxYzq4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.geeksforgeeks.org/scrape-google-ngram-viewer-using-python/\n",
        "import urllib\n",
        "import requests\n",
        "import json\n",
        "def runQuery(query, start_year, \n",
        "             end_year, corpus,\n",
        "             smoothing):\n",
        "  \n",
        "    # converting a regular string to \n",
        "    # the standard URL format\n",
        "    # eg: \"geeks for,geeks\" will\n",
        "    # convert to \"geeks%20for%2Cgeeks\"\n",
        "    out_dict = {}\n",
        "    query = urllib.parse.quote(query)\n",
        "  \n",
        "    # creating the URL\n",
        "    url = \"https://books.google.com/ngrams/json?content=\" + query +\"&year_start=\" + str(start_year) + \"&year_end=\" + str(end_year) + \"&corpus=\" + str(corpus) + \"&smoothing=\" + str(smoothing) + \"\"\n",
        "  \n",
        "    # requesting data from the above url\n",
        "    response1 = requests.get(url)\n",
        "    \n",
        "    # extracting the json data from the response we got\n",
        "    output = response1.json()\n",
        "    #print(output)\n",
        "    for num in range(len(output)):\n",
        "      if len(output)== 0:\n",
        "        continue\n",
        "      else:\n",
        "        #return(list(output[num]['timeseries']))\n",
        "        out_dict[output[num]['ngram']] = list(output[num]['timeseries'])\n",
        "      return out_dict\n",
        " \n",
        "    \n",
        "      \n",
        "\n",
        "     # y = (output[num]['timeseries'])\n",
        "     # array = (np.array(y))\n",
        "     # return array\n",
        "      "
      ],
      "metadata": {
        "id": "tTGBUASRz8ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I forget how I made this list \n",
        "# I think I just took the words from the lemmas column of my giant connlu dataframe\n",
        "#filtering punctuation etc \n",
        "\n",
        "lemmas = []\n",
        "for item in lems:\n",
        "  if item in lemmas:\n",
        "    continue\n",
        "  elif item == '.':\n",
        "    continue\n",
        "  elif item == ',':\n",
        "    continue\n",
        "  elif item == ';':\n",
        "    continue\n",
        "  else:\n",
        "    lemmas.append(item)\n",
        "print(lemmas)"
      ],
      "metadata": {
        "id": "tEhws0Q-0fY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making a directory for the csv files of ngrams data\n",
        "!mkdir ngrams_data"
      ],
      "metadata": {
        "id": "EsDRHjMa0rCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in lemmas:\n",
        "  try:\n",
        "    x = runQuery(item, start_year=1700, end_year=1900, corpus=31, smoothing=0)\n",
        "  except Exception as e:\n",
        "    pass\n",
        "    #gets only numbers from google ngrams output\n",
        "    ngdata =pd.DataFrame.from_dict(x, orient='columns') \n",
        "      #not the most efficient but- makes an array with the word to attach it to data\n",
        "    dayu= np.repeat(item,201)\n",
        "    gram= np.reshape(dayu,(201,1))\n",
        "    ls = pd.DataFrame(gram)\n",
        "    final = pd.concat([ls,ngdata], axis= 1)\n",
        "      #this gives one column of the word, and one column of the ngrams\n",
        "      #altogether, this dataframe will be whats on the y axis of heatmap\n",
        "    final.rename(columns = {0:'Ngram'}, inplace = True)\n",
        "      #adds the year range which is in the same order as data scraped from google ngrams\n",
        "    list1 = list(np.arange(1700,1900+1))\n",
        "    yrs =pd.DataFrame(list1)\n",
        "    yrs.rename(columns={0:'Years'}, inplace=True )\n",
        "      #adds 2 dataframes together\n",
        "    horizontal_stack = pd.concat([final, yrs], axis=1)\n",
        "    horizontal_stack.rename(columns={ horizontal_stack.columns[1]: \"NGdata\" }, inplace = True)\n",
        "      #write to csv\n",
        "    horizontal_stack.to_csv(f\"/content/ngrams_data/{item}.csv\",)"
      ],
      "metadata": {
        "id": "QNYHrvKg00BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ngrams_data file can be found here : https://github.com/jdeen33/Altertumswissenschaft-/blob/main/wolfngrams.csv "
      ],
      "metadata": {
        "id": "JyfZPr8D1AXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#then used bins and pivot tables to organize data. Data organized in 10 year increments for each word, from the years 1700-1900\n",
        "#splitting data for years into intervals of 10\n",
        "bins = [1700,1710,1720,1730,1740,1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1900]\n",
        "wolf['years'] = pd.cut(wolf['Years'], bins)\n",
        "wolf"
      ],
      "metadata": {
        "id": "UccjEcns1gtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "binned csv can be found here:\n",
        "https://github.com/jdeen33/Altertumswissenschaft-/blob/main/pivotwolfngrams.csv"
      ],
      "metadata": {
        "id": "loe0bf0m14Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making a heatmap \n",
        "from matplotlib.pyplot import figure\n",
        "df_wide = wolf.pivot_table( index= 'Ngram' ,columns='years', values='NGdata')\n",
        "l = df_wide.sample(n=100) #shows a random sample of 100 words \n",
        "figure(figsize=(10, 8), dpi=100)\n",
        "# plot it\n",
        "sns.heatmap(l)"
      ],
      "metadata": {
        "id": "WUw8iOQA1w4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "heatmap example can be found here:\n",
        "https://github.com/jdeen33/Altertumswissenschaft-/blob/main/ngrams_heatmap.png "
      ],
      "metadata": {
        "id": "CxOq--rm164b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using DeepL API to automate an english translation "
      ],
      "metadata": {
        "id": "-KKyy3692wj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports \n",
        "import toolz\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#used new XML (wolf_attributes- link to github file above)\n",
        "!pip install --upgrade deepl\n",
        "import deepl"
      ],
      "metadata": {
        "id": "VQSSy7aN20hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wolf= open('wolf_attributes.xml')\n",
        "wolf_soup = BeautifulSoup(wolf,'lxml')\n",
        "\n",
        "\n",
        "sents = wolf_soup.find_all('s')\n",
        "s_dict= {}\n",
        "for item in sents:\n",
        "  s_dict[item['id']] = item.get_text()\n"
      ],
      "metadata": {
        "id": "-UWadZGk3HC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data cleaning \n",
        "import re\n",
        "pattern = \"\\n\\t\\t\\t\"\n",
        "pattern2 = \"\\s\\\\n*\"\n",
        "def cleanxml(x):\n",
        "  #return re.sub(pattern, \" \", x)\n",
        "  return re.sub(pattern2, \" \", x)\n",
        "\n",
        "#removing all the line and tab breaks\n",
        "def map2dictionary(x,d):\n",
        "  cleandict = toolz.valmap(cleanxml,s_dict)\n",
        "  return cleandict"
      ],
      "metadata": {
        "id": "oc2VaOXq3Kxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wolf_dict = map2dictionary(cleanxml,s_dict)"
      ],
      "metadata": {
        "id": "wBA2_wnz3Ugl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean data to run through api \n",
        "wolf_dict"
      ],
      "metadata": {
        "id": "-i-_IfN43ZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deepL is unfortunately behind a paywall but I used a free subscription \n",
        "auth_key = \"a4713cc6-2a17-9891-4c5a-8ef473dd80dd\"\n",
        "translator = deepl.Translator(auth_key)"
      ],
      "metadata": {
        "id": "DAigmQUi3f82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using dict comprehension to translate each sentence \n",
        "def translate_dict(dict_):\n",
        "  wolf_eng = {}\n",
        "  for key in dict_:\n",
        "    if len(dict_[key]) > 0 :\n",
        "      result= translator.translate_text(dict_[key],target_lang=\"EN-US\")\n",
        "      wolf_eng[key] = result.text\n",
        "  return wolf_eng\n",
        "#concat into dataframe then download as csv file \n",
        "wolf_pd = pd.DataFrame.from_dict(translate_dict(wolf_dict), orient='index')\n",
        "wolf_pd.to_csv('wolf_automated_eng', encoding = 'utf-8-sig') \n",
        "files.download('wolf_automated_eng')\n"
      ],
      "metadata": {
        "id": "y6XL7Juy3o7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to this csv can be found here:\n",
        "https://github.com/jdeen33/Altertumswissenschaft-/blob/main/wolf_automated_eng.txt "
      ],
      "metadata": {
        "id": "bs-8FhU832LC"
      }
    }
  ]
}